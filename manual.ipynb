{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c17eb5f9",
   "metadata": {},
   "source": [
    "# Manual Gaussian Naive Bayes (No scikit-learn)\n",
    "\n",
    "This notebook re-implements a Gaussian Naive Bayes classifier from scratch to compare performance with vs. without the engineered `purity_score` feature.\n",
    "\n",
    "Goals:\n",
    "1) Load `fetal_health_with_purity.csv`\n",
    "2) Implement utilities: stratified split, metrics (accuracy, confusion matrix, per-class precision/recall/F1)\n",
    "3) Implement Gaussian Naive Bayes manually: priors, per-class feature means/variances, and log-likelihood scoring with numerical stability\n",
    "4) Run two experiments: with `purity_score` and without\n",
    "5) Compare metrics and optionally persist the better model\n",
    "\n",
    "Notes:\n",
    "- No scikit-learn is used for modeling or splitting; only pandas, numpy, and standard library\n",
    "- Variance smoothing (epsilon) added to avoid divide-by-zero\n",
    "- Computations are vectorized for clarity and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53110795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Manual Gaussian Naive Bayes training & evaluation ===\n",
    "# - No scikit-learn\n",
    "# - Two experiments: with and without `purity_score`\n",
    "\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# -----------------------------\n",
    "# Data loading\n",
    "# -----------------------------\n",
    "print(\"Loading dataset with purity score...\")\n",
    "df = pd.read_csv('fetal_health_with_purity.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "target_col = 'fetal_health' if 'fetal_health' in df.columns else df.columns[-1]\n",
    "feature_cols_all = [c for c in df.columns if c != target_col]\n",
    "if 'purity_score' not in feature_cols_all:\n",
    "    raise RuntimeError(\"purity_score not found. Ensure the feature engineering notebook saved fetal_health_with_purity.csv.\")\n",
    "\n",
    "feature_cols_without = [c for c in feature_cols_all if c != 'purity_score']\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities: stratified split\n",
    "# -----------------------------\n",
    "def stratified_train_test_split(X: np.ndarray, y: np.ndarray, test_size=0.2, random_state=42):\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    y = np.asarray(y)\n",
    "    classes = np.unique(y)\n",
    "    train_idx, test_idx = [], []\n",
    "    for c in classes:\n",
    "        idx = np.where(y == c)[0]\n",
    "        rng.shuffle(idx)\n",
    "        n_test = max(1, int(round(len(idx) * test_size)))\n",
    "        test_idx.extend(idx[:n_test].tolist())\n",
    "        train_idx.extend(idx[n_test:].tolist())\n",
    "    # Shuffle final indices for randomness\n",
    "    rng.shuffle(train_idx)\n",
    "    rng.shuffle(test_idx)\n",
    "    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics: accuracy, confusion matrix, report\n",
    "# -----------------------------\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    return (y_true == y_pred).mean()\n",
    "\n",
    "def confusion_matrix_(y_true, y_pred, labels=None):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    if labels is None:\n",
    "        labels = np.unique(np.concatenate([y_true, y_pred]))\n",
    "    label_to_idx = {lbl: i for i, lbl in enumerate(labels)}\n",
    "    cm = np.zeros((len(labels), len(labels)), dtype=int)\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        cm[label_to_idx[t], label_to_idx[p]] += 1\n",
    "    return cm, labels\n",
    "\n",
    "def classification_report_(y_true, y_pred, labels=None):\n",
    "    cm, labels = confusion_matrix_(y_true, y_pred, labels)\n",
    "    report_rows = []\n",
    "    for i, lbl in enumerate(labels):\n",
    "        tp = cm[i, i]\n",
    "        fp = cm[:, i].sum() - tp\n",
    "        fn = cm[i, :].sum() - tp\n",
    "        prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        rec = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "        support = cm[i, :].sum()\n",
    "        report_rows.append((lbl, prec, rec, f1, int(support)))\n",
    "    # Macro averages\n",
    "    macro_prec = np.mean([r[1] for r in report_rows])\n",
    "    macro_rec = np.mean([r[2] for r in report_rows])\n",
    "    macro_f1 = np.mean([r[3] for r in report_rows])\n",
    "    return report_rows, (macro_prec, macro_rec, macro_f1)\n",
    "\n",
    "# -----------------------------\n",
    "# Manual Gaussian Naive Bayes\n",
    "# -----------------------------\n",
    "class ManualGaussianNB:\n",
    "    def __init__(self, var_smoothing=1e-9, prior_smoothing=0.0):\n",
    "        self.var_smoothing = var_smoothing\n",
    "        self.prior_smoothing = prior_smoothing\n",
    "        self.classes_ = None\n",
    "        self.class_log_prior_ = None\n",
    "        self.theta_ = None  # means per class per feature [n_classes, n_features]\n",
    "        self.sigma_ = None  # variances per class per feature [n_classes, n_features]\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        y = np.asarray(y)\n",
    "        self.classes_, y_idx = np.unique(y, return_inverse=True)\n",
    "        n_classes = len(self.classes_)\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        # Priors with optional smoothing\n",
    "        class_counts = np.bincount(y_idx, minlength=n_classes).astype(float)\n",
    "        if self.prior_smoothing > 0:\n",
    "            class_counts += self.prior_smoothing\n",
    "        class_priors = class_counts / class_counts.sum()\n",
    "        self.class_log_prior = np.log(class_priors)\n",
    "\n",
    "        # Means and variances per class\n",
    "        theta = np.zeros((n_classes, n_features), dtype=float)\n",
    "        sigma = np.zeros((n_classes, n_features), dtype=float)\n",
    "        for c_i in range(n_classes):\n",
    "            Xi = X[y_idx == c_i]\n",
    "            if Xi.shape[0] == 0:\n",
    "                # Handle empty class (should not happen with stratified split)\n",
    "                theta[c_i, :] = 0.0\n",
    "                sigma[c_i, :] = 1.0\n",
    "            else:\n",
    "                theta[c_i, :] = Xi.mean(axis=0)\n",
    "                # Unbiased variance can be noisy for tiny classes; use population variance\n",
    "                v = Xi.var(axis=0)\n",
    "                # Variance smoothing to avoid zero\n",
    "                v = np.maximum(v, self.var_smoothing)\n",
    "                sigma[c_i, :] = v\n",
    "        self.theta_ = theta\n",
    "        self.sigma_ = sigma\n",
    "        return self\n",
    "\n",
    "    def _joint_log_likelihood(self, X: np.ndarray):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        # Using Gaussian log pdf per feature and summing (naive independence)\n",
    "        # log P(x|c) = sum_j [ -0.5*log(2*pi*sigma_cj) - (x_j - mu_cj)^2 / (2*sigma_cj) ]\n",
    "        n_classes, n_features = self.theta_.shape\n",
    "        # Precompute constants\n",
    "        log_prob = []\n",
    "        for c_i in range(n_classes):\n",
    "            mu = self.theta_[c_i]\n",
    "            var = self.sigma_[c_i]\n",
    "            # Avoid division by zero (already smoothed)\n",
    "            log_det = -0.5 * np.sum(np.log(2.0 * np.pi * var))\n",
    "            # Quadratic term\n",
    "            quad = -0.5 * np.sum(((X - mu) ** 2) / var, axis=1)\n",
    "            log_prob.append(self.class_log_prior[c_i] + log_det + quad)\n",
    "        # Shape -> [n_samples, n_classes]\n",
    "        return np.vstack(log_prob).T\n",
    "\n",
    "    def predict(self, X: np.ndarray):\n",
    "        jll = self._joint_log_likelihood(X)\n",
    "        idx = np.argmax(jll, axis=1)\n",
    "        return self.classes_[idx]\n",
    "\n",
    "    def predict_proba(self, X: np.ndarray):\n",
    "        jll = self._joint_log_likelihood(X)\n",
    "        # Convert log-likelihoods to probabilities with log-sum-exp stabilization\n",
    "        max_log = np.max(jll, axis=1, keepdims=True)\n",
    "        exp_shifted = np.exp(jll - max_log)\n",
    "        probs = exp_shifted / np.sum(exp_shifted, axis=1, keepdims=True)\n",
    "        return probs\n",
    "\n",
    "# -----------------------------\n",
    "# Prepare data and run experiments\n",
    "# -----------------------------\n",
    "# Ensure numeric target if needed\n",
    "labels = df[target_col].values\n",
    "# If target is float but actually categorical like 1.0, 2.0, 3.0, keep as-is\n",
    "\n",
    "# Two feature matrices\n",
    "X_with = df[feature_cols_all].values.astype(float)\n",
    "X_without = df[feature_cols_without].values.astype(float)\n",
    "\n",
    "# Split with stratification\n",
    "Xw_tr, Xw_te, yw_tr, yw_te = stratified_train_test_split(X_with, labels, test_size=0.2, random_state=42)\n",
    "Xo_tr, Xo_te, yo_tr, yo_te = stratified_train_test_split(X_without, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train models\n",
    "nb_with = ManualGaussianNB(var_smoothing=1e-9, prior_smoothing=0.0).fit(Xw_tr, yw_tr)\n",
    "nb_without = ManualGaussianNB(var_smoothing=1e-9, prior_smoothing=0.0).fit(Xo_tr, yo_tr)\n",
    "\n",
    "# Predictions\n",
    "yp_with = nb_with.predict(Xw_te)\n",
    "yp_without = nb_without.predict(Xo_te)\n",
    "\n",
    "# Metrics\n",
    "acc_with = accuracy(yw_te, yp_with)\n",
    "acc_without = accuracy(yo_te, yp_without)\n",
    "\n",
    "rep_with, macro_with = classification_report_(yw_te, yp_with)\n",
    "rep_without, macro_without = classification_report_(yo_te, yp_without)\n",
    "\n",
    "cm_with, labels_with = confusion_matrix_(yw_te, yp_with)\n",
    "cm_without, labels_without = confusion_matrix_(yo_te, yp_without)\n",
    "\n",
    "print(\"\\n=== Results: With purity_score ===\")\n",
    "print(f\"Accuracy: {acc_with:.4f}\")\n",
    "print(\"Per-class (label, precision, recall, f1, support):\")\n",
    "for row in rep_with:\n",
    "    print(row)\n",
    "print(f\"Macro avg (precision, recall, f1): {macro_with}\")\n",
    "print(\"Confusion matrix (rows=true, cols=pred):\\n\", cm_with)\n",
    "\n",
    "print(\"\\n=== Results: Without purity_score ===\")\n",
    "print(f\"Accuracy: {acc_without:.4f}\")\n",
    "print(\"Per-class (label, precision, recall, f1, support):\")\n",
    "for row in rep_without:\n",
    "    print(row)\n",
    "print(f\"Macro avg (precision, recall, f1): {macro_without}\")\n",
    "print(\"Confusion matrix (rows=true, cols=pred):\\n\", cm_without)\n",
    "\n",
    "# -----------------------------\n",
    "# Save best model (optional)\n",
    "# -----------------------------\n",
    "best_tag = 'with_purity' if acc_with >= acc_without else 'without_purity'\n",
    "best_model = nb_with if best_tag == 'with_purity' else nb_without\n",
    "os.makedirs('model', exist_ok=True)\n",
    "model_path = f\"model/gaussian_nb_manual_{best_tag}.pkl\"\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'model': best_model,\n",
    "        'feature_cols': feature_cols_all if best_tag=='with_purity' else feature_cols_without,\n",
    "        'target_col': target_col,\n",
    "        'var_smoothing': best_model.var_smoothing,\n",
    "        'classes_': best_model.classes_\n",
    "    }, f)\n",
    "print(f\"\\nSaved best manual model to: {model_path}\")\n",
    "\n",
    "# Quick sanity predictions on a few samples\n",
    "n_show = min(5, len(Xw_te))\n",
    "idxs = np.random.choice(len(Xw_te), n_show, replace=False)\n",
    "print(\"\\nSample predictions (with purity): True vs Pred\")\n",
    "for i in idxs:\n",
    "    print(int(yw_te[i]), int(yp_with[i]))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
