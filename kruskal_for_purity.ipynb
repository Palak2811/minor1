{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc8e25c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Novel MST-Based Purity Score Algorithm ---\n",
      "This script will add a new 'purity_score' feature to the fetal health dataset.\n",
      "\n",
      "Loading fetal_health.csv...\n",
      "Loaded 2126 records with 21 features.\n",
      "Standardizing features...\n",
      "Calculating all-pairs distances (total 2258875 edges)...\n",
      "  -> Edge calculation took 8.78 seconds.\n",
      "Sorting all edges...\n",
      "  -> Sorting took 2.58 seconds.\n",
      "Building Minimum Spanning Tree (MST)...\n",
      "  -> MST build took 1.58 seconds.\n",
      "Calculating novel Purity Scores...\n",
      "Saving new dataset...\n",
      "\n",
      "--- SUCCESS! ---\n",
      "A new file 'fetal_health_with_purity.csv' has been created.\n",
      "It contains your original data plus the new 'purity_score' feature.\n",
      "\n",
      "Sample of results (Low score = boundary, High score = core):\n",
      "             purity_score                                          \\\n",
      "                    count           mean            std       min   \n",
      "fetal_health                                                        \n",
      "1.0                1655.0  918429.063159  273792.537224  0.036882   \n",
      "2.0                 295.0  494915.738092  500823.239271  0.047399   \n",
      "3.0                 176.0  710227.495699  454950.767611  0.140756   \n",
      "\n",
      "                                                                    \n",
      "                         25%             50%        75%        max  \n",
      "fetal_health                                                        \n",
      "1.0           1000000.000000  1000000.000000  1000000.0  1000000.0  \n",
      "2.0                 0.659248        6.313786  1000000.0  1000000.0  \n",
      "3.0                 1.245805  1000000.000000  1000000.0  1000000.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "print(\"--- Novel MST-Based Purity Score Algorithm ---\")\n",
    "print(\"This script will add a new 'purity_score' feature to the fetal health dataset.\")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# MODULE 1: DISJOINT SET UNION (DSU) STRUCTURE\n",
    "# This is required to detect cycles in Kruskal's algorithm\n",
    "# -----------------------------------------------------------------\n",
    "class DSU:\n",
    "    \"\"\"\n",
    "    A class for the Disjoint Set Union (DSU) data structure.\n",
    "    Also known as Union-Find.\n",
    "    \"\"\"\n",
    "    def __init__(self, n):\n",
    "        # Initialize parent array, each node is its own parent\n",
    "        self.parent = list(range(n))\n",
    "\n",
    "    def find(self, i):\n",
    "        \"\"\"Finds the root representative of node i's set (with path compression)\"\"\"\n",
    "        if self.parent[i] == i:\n",
    "            return i\n",
    "        # Path compression: set parent directly to the root\n",
    "        self.parent[i] = self.find(self.parent[i])\n",
    "        return self.parent[i]\n",
    "\n",
    "    def union(self, i, j):\n",
    "        \"\"\"Merges the sets containing nodes i and j\"\"\"\n",
    "        root_i = self.find(i)\n",
    "        root_j = self.find(j)\n",
    "        \n",
    "        if root_i != root_j:\n",
    "            # Not in the same set, merge them\n",
    "            self.parent[root_i] = root_j\n",
    "            return True  # Successful union\n",
    "        \n",
    "        return False # Already in the same set (cycle detected)\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# MODULE 2: DATA LOADING AND PREPARATION\n",
    "# -----------------------------------------------------------------\n",
    "try:\n",
    "    print(\"\\nLoading fetal_health.csv...\")\n",
    "    df = pd.read_csv('fetal_health.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'fetal_health.csv' not found.\")\n",
    "    print(\"Please place this script in the same folder as your dataset.\")\n",
    "    exit()\n",
    "\n",
    "# Separate features (X) and labels (y)\n",
    "# We assume the last column is the target\n",
    "features_df = df.iloc[:, :-1]\n",
    "labels = df.iloc[:, -1].values\n",
    "n = len(df) # Number of records (nodes)\n",
    "d = features_df.shape[1] # Number of features (dimensions)\n",
    "\n",
    "print(f\"Loaded {n} records with {d} features.\")\n",
    "\n",
    "# CRITICAL STEP: Standardize features\n",
    "# Distances are only meaningful if features are on the same scale.\n",
    "print(\"Standardizing features...\")\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features_df)\n",
    "\n",
    "# Epsilon for stable division\n",
    "epsilon = 1e-6 \n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# MODULE 3: MAIN ALGORITHM SCRIPT\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# --- 3A: Calculate all edge weights and sort them ---\n",
    "print(f\"Calculating all-pairs distances (total {n*(n-1)//2} edges)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "all_edges = []\n",
    "for i in range(n):\n",
    "    for j in range(i + 1, n):\n",
    "        # Calculate Euclidean distance using fast numpy operations\n",
    "        dist = np.sqrt(np.sum((features_scaled[i] - features_scaled[j])**2))\n",
    "        all_edges.append((dist, i, j)) # (weight, node1, node2)\n",
    "\n",
    "print(f\"  -> Edge calculation took {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "# --- 3B: Sort edges by weight ---\n",
    "print(\"Sorting all edges...\")\n",
    "start_time = time.time()\n",
    "all_edges.sort()\n",
    "print(f\"  -> Sorting took {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "# --- 3C: Build the MST using Kruskal's Algorithm ---\n",
    "print(\"Building Minimum Spanning Tree (MST)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "dsu = DSU(n)\n",
    "# This will store the MST structure: {node: [(neighbor, dist), ...]}\n",
    "mst_adj = {i: [] for i in range(n)} \n",
    "mst_edge_count = 0\n",
    "\n",
    "for dist, u, v in all_edges:\n",
    "    if dsu.union(u, v):\n",
    "        # No cycle! Accept this edge.\n",
    "        mst_adj[u].append((v, dist))\n",
    "        mst_adj[v].append((u, dist))\n",
    "        mst_edge_count += 1\n",
    "        \n",
    "        # Stop once the MST is complete\n",
    "        if mst_edge_count == n - 1:\n",
    "            break\n",
    "\n",
    "print(f\"  -> MST build took {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "# --- 3D: Calculate the Purity Score for each node ---\n",
    "print(\"Calculating novel Purity Scores...\")\n",
    "purity_scores = []\n",
    "for i in range(n):\n",
    "    penalty = 0\n",
    "    my_label = labels[i]\n",
    "    \n",
    "    # Get neighbors *from the MST only*\n",
    "    for neighbor_id, dist in mst_adj[i]:\n",
    "        neighbor_label = labels[neighbor_id]\n",
    "        \n",
    "        # If neighbor's class is DIFFERENT, add the distance as a penalty\n",
    "        if neighbor_label != my_label:\n",
    "            penalty += dist\n",
    "            \n",
    "    # Final score is the inverse of the penalty\n",
    "    score = 1.0 / (penalty + epsilon)\n",
    "    purity_scores.append(score)\n",
    "\n",
    "# --- 4: Save Results ---\n",
    "print(\"Saving new dataset...\")\n",
    "# Add the new feature to our original dataframe\n",
    "df['purity_score'] = purity_scores\n",
    "\n",
    "# Save to a new CSV file\n",
    "output_filename = 'fetal_health_with_purity.csv'\n",
    "df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"\\n--- SUCCESS! ---\")\n",
    "print(f\"A new file '{output_filename}' has been created.\")\n",
    "print(\"It contains your original data plus the new 'purity_score' feature.\")\n",
    "\n",
    "# Display a sample of the results\n",
    "print(\"\\nSample of results (Low score = boundary, High score = core):\")\n",
    "print(df[['fetal_health', 'purity_score']].groupby('fetal_health').describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e18f4a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset with purity score...\n",
      "Dataset shape: (2126, 23)\n",
      "\n",
      "=== Results: With purity_score ===\n",
      "Accuracy: 0.8192\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         1.0     0.9826    0.8494    0.9111       332\n",
      "         2.0     0.4587    0.8475    0.5952        59\n",
      "         3.0     0.5667    0.4857    0.5231        35\n",
      "\n",
      "    accuracy                         0.8192       426\n",
      "   macro avg     0.6693    0.7275    0.6765       426\n",
      "weighted avg     0.8759    0.8192    0.8355       426\n",
      "\n",
      "Confusion matrix:\n",
      " [[282  43   7]\n",
      " [  3  50   6]\n",
      " [  2  16  17]]\n",
      "\n",
      "=== Results: Without purity_score ===\n",
      "Accuracy: 0.8099\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         1.0     0.9691    0.8494    0.9053       332\n",
      "         2.0     0.4476    0.7966    0.5732        59\n",
      "         3.0     0.5333    0.4571    0.4923        35\n",
      "\n",
      "    accuracy                         0.8099       426\n",
      "   macro avg     0.6500    0.7011    0.6569       426\n",
      "weighted avg     0.8611    0.8099    0.8254       426\n",
      "\n",
      "Confusion matrix:\n",
      " [[282  42   8]\n",
      " [  6  47   6]\n",
      " [  3  16  16]]\n",
      "\n",
      "Saved best model (with purity_score) to: model/gaussian_nb_with_purity.joblib\n",
      "Saved corresponding scaler to: model/scaler_with_purity.joblib\n",
      "\n",
      "Sample predictions (test set):\n",
      "True\tPredicted\n",
      "[(1, 1), (1, 3), (1, 1), (1, 2), (1, 1)]\n"
     ]
    }
   ],
   "source": [
    "# === Model training & evaluation: Gaussian Naive Bayes ===\n",
    "# This cell loads the CSV produced earlier (with `purity_score`),\n",
    "# runs two experiments (with and without purity_score), prints metrics,\n",
    "# and saves the model that uses the purity_score.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "print('\\nLoading dataset with purity score...')\n",
    "df_all = pd.read_csv('fetal_health_with_purity.csv')\n",
    "print(f\"Dataset shape: {df_all.shape}\")\n",
    "\n",
    "# Identify target column (assume same as before)\n",
    "target_col = 'fetal_health' if 'fetal_health' in df_all.columns else df_all.columns[-1]\n",
    "\n",
    "# Prepare feature sets\n",
    "feature_cols = [c for c in df_all.columns if c != target_col]\n",
    "if 'purity_score' not in feature_cols:\n",
    "    raise RuntimeError(\"purity_score column not found in 'fetal_health_with_purity.csv'. Please ensure the previous cell ran and saved the file.\")\n",
    "\n",
    "# Original-features-only (exclude purity_score)\n",
    "orig_features = [c for c in feature_cols if c != 'purity_score']\n",
    "\n",
    "X_with = df_all[feature_cols].copy()\n",
    "X_without = df_all[orig_features].copy()\n",
    "y = df_all[target_col].values\n",
    "\n",
    "# Train/test split (stratify if labels are categorical)\n",
    "Xw_train, Xw_test, yw_train, yw_test = train_test_split(X_with, y, test_size=0.2, random_state=42, stratify=y)\n",
    "Xo_train, Xo_test, yo_train, yo_test = train_test_split(X_without, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features (GaussianNB assumes continuous features; scaling often helps)\n",
    "scaler_with = StandardScaler()\n",
    "Xw_train_s = scaler_with.fit_transform(Xw_train)\n",
    "Xw_test_s = scaler_with.transform(Xw_test)\n",
    "\n",
    "scaler_without = StandardScaler()\n",
    "Xo_train_s = scaler_without.fit_transform(Xo_train)\n",
    "Xo_test_s = scaler_without.transform(Xo_test)\n",
    "\n",
    "# Helper to train & evaluate\n",
    "def train_eval(X_train, X_test, y_train, y_test, desc=''):\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, digits=4)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"\\n=== Results: {desc} ===\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(\"\\nClassification report:\\n\", report)\n",
    "    print(\"Confusion matrix:\\n\", cm)\n",
    "    return clf, acc\n",
    "\n",
    "# Run experiments\n",
    "clf_with, acc_with = train_eval(Xw_train_s, Xw_test_s, yw_train, yw_test, desc='With purity_score')\n",
    "clf_without, acc_without = train_eval(Xo_train_s, Xo_test_s, yo_train, yo_test, desc='Without purity_score')\n",
    "\n",
    "# Compare and save the model that performed better (prefer with purity if tie)\n",
    "if acc_with >= acc_without:\n",
    "    best_clf = clf_with\n",
    "    best_scaler = scaler_with\n",
    "    model_name = 'model/gaussian_nb_with_purity.joblib'\n",
    "    scaler_name = 'model/scaler_with_purity.joblib'\n",
    "    chosen = 'with purity_score'\n",
    "else:\n",
    "    best_clf = clf_without\n",
    "    best_scaler = scaler_without\n",
    "    model_name = 'model/gaussian_nb_without_purity.joblib'\n",
    "    scaler_name = 'model/scaler_without_purity.joblib'\n",
    "    chosen = 'without purity_score'\n",
    "\n",
    "# Ensure model dir exists and save\n",
    "import os\n",
    "os.makedirs('model', exist_ok=True)\n",
    "joblib.dump(best_clf, model_name)\n",
    "joblib.dump(best_scaler, scaler_name)\n",
    "\n",
    "print(f\"\\nSaved best model ({chosen}) to: {model_name}\")\n",
    "print(f\"Saved corresponding scaler to: {scaler_name}\")\n",
    "\n",
    "# Quick sample of predictions on test set for sanity\n",
    "sample_idx = np.random.choice(len(Xw_test_s), min(5, len(Xw_test_s)), replace=False)\n",
    "print('\\nSample predictions (test set):')\n",
    "print('True\\tPredicted')\n",
    "print([ (int(yw_test[i]), int(clf_with.predict(Xw_test_s)[i])) for i in sample_idx ])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
